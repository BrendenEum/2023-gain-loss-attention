---
title: "Attentional Biases in Gain-Loss Lotteries"
author: Brenden Eum
date: May 19, 2022
output: 
  html_document: 
    theme: united
    toc: yes
    toc_float: yes
    toc_depth: 2
    number_sections: yes
    code_folding: hide
    df_print: default
---

# Introduction

Attentional discounting flips from positive to negative between appetitive and aversive outcomes. Is this also true for lotteries in the gain and loss domains?

In order for this notebook to run, the following folder structure needs to be mimicked:

* 00 exploratory
  + figures
  + output
  + temp
* 01 confirmatory
  + figures
  + output
  + temp
* 02 joint
  + figures
  + output
  + temp
* data
  + good
    - (a separate folder for each subject)
* code (where this notebook should be placed)

--------------------------------------------------------------------------------------------------------

# Preparation {.tabset}

## Load

Load libraries and start fresh.

```{r}
#load("D:/OneDrive - California Institute of Technology/PhD/Rangel Lab/OLD/2022-gain-loss-attention/00 exploratory/output/cfr.RData")
```

```{r}
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/brms/brms_2.13.5.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
```



```{r load, message=FALSE, warning=FALSE}

rm(list = ls())
seed = 1337
library(dplyr) #pipeline %>%
library(plotrix) #std.error
library(ggplot2) #plots... duh
library(grid) #combine plots
library(gridExtra) #more combining plots
library(ggsci) #I like the Futurama color palette <3
#library(brms) #hierarchical regressions
#library(runjags) #MCMC
#library(cmdstanr) #backend
#set_cmdstan_path("D:/Program Files/R-4.0.2/library/cmdstan-2.24.1") #gotta do this everytime :\
#library(R2jags) #interface between R and jags
library(coda) #simulate mcmc
library(bayesplot) #basic PPC stuff
#library(CalvinBayes) #fancy PPC stuff
library(effsize) #cohen's big fat d
library(latex2exp) #for fancy math stuffs in the plots
#library(ggpubr) #column titles for grid.arrange plots

datadir <- 'D:\\OneDrive - California Institute of Technology\\PhD\\Rangel Lab\\OLD\\2022-gain-loss-attention\\data\\good' #the only directory you need to set manually

quickdataclean <- T #skip data cleaning process?
quickmodel <- T #skip model fitting and use old estimates
quickreg <- T #skip regressions
show.reg.progress <- 0 #1=yes, 0=no

load("D:/OneDrive - California Institute of Technology/PhD/Rangel Lab/OLD/2022-gain-loss-attention/00 exploratory/output/cfr.RData")
```

## Split

Gather all the data and allocate to either exploratory / confirmatory datasets.

```{r split}

if (!quickdataclean) {
  
  raw_subs <- list.files(path=datadir)
  exploratory_subs <- raw_subs[1:floor(length(raw_subs)/2)]
  confirmatory_subs <- raw_subs[(floor(length(raw_subs)/2)+1):length(raw_subs)]
  
  ######################
  ## Exploratory
  ######################
  
  source("set_e_dir.R")
  
  choices_gain   <- read.csv(file.path(datadir,exploratory_subs[1],  paste0("choice_",exploratory_subs[1],"_win.csv")))
  choices_loss   <- read.csv(file.path(datadir,exploratory_subs[1],  paste0("choice_",exploratory_subs[1],"_loss.csv")))
  fixations_gain <- read.csv(file.path(datadir,exploratory_subs[1],paste0("cleanFix_",exploratory_subs[1],"_win_corr.csv")))
  fixations_loss <- read.csv(file.path(datadir,exploratory_subs[1],paste0("cleanFix_",exploratory_subs[1],"_loss_corr.csv")))
  
  for (raw_sub in raw_subs[2:length(exploratory_subs)]) {
    choices_gain   <- rbind(choices_gain,    read.csv(file.path(datadir,raw_sub,paste0(  "choice_",raw_sub,"_win.csv"))))
    choices_loss   <- rbind(choices_loss,   read.csv(file.path(datadir,raw_sub,paste0(  "choice_",raw_sub,"_loss.csv"))))
    fixations_gain <- rbind(fixations_gain,  read.csv(file.path(datadir,raw_sub,paste0("cleanFix_",raw_sub,"_win_corr.csv"))))
    fixations_loss <- rbind(fixations_loss, read.csv(file.path(datadir,raw_sub,paste0("cleanFix_",raw_sub,"_loss_corr.csv"))))
  }
  fixations_gain$condition <- "win" #fixations are missing condition variable
  fixations_loss$condition <- "loss"
  
  choices   <- rbind(choices_gain, choices_loss)
  fixations <- rbind(fixations_gain, fixations_loss)
  
  save(choices, file=file.path(outdir,"raw_choices.RData"))
  save(fixations, file=file.path(outdir,"raw_fixations.RData"))
  
  ######################
  ## Confirmatory
  ######################
  
  source("set_c_dir.R")
  
  choices_gain   <- read.csv(file.path(datadir,confirmatory_subs[1],  paste0("choice_",confirmatory_subs[1],"_win.csv")))
  choices_loss   <- read.csv(file.path(datadir,confirmatory_subs[1],  paste0("choice_",confirmatory_subs[1],"_loss.csv")))
  fixations_gain <- read.csv(file.path(datadir,confirmatory_subs[1],paste0("cleanFix_",confirmatory_subs[1],"_win_corr.csv")))
  fixations_loss <- read.csv(file.path(datadir,confirmatory_subs[1],paste0("cleanFix_",confirmatory_subs[1],"_loss_corr.csv")))
  
  for (raw_sub in raw_subs[2:length(confirmatory_subs)]) {
    choices_gain   <- rbind(choices_gain,    read.csv(file.path(datadir,raw_sub,paste0(  "choice_",raw_sub,"_win.csv"))))
    choices_loss   <- rbind(choices_loss,   read.csv(file.path(datadir,raw_sub,paste0(  "choice_",raw_sub,"_loss.csv"))))
    fixations_gain <- rbind(fixations_gain,  read.csv(file.path(datadir,raw_sub,paste0("cleanFix_",raw_sub,"_win_corr.csv"))))
    fixations_loss <- rbind(fixations_loss, read.csv(file.path(datadir,raw_sub,paste0("cleanFix_",raw_sub,"_loss_corr.csv"))))
  }
  fixations_gain$condition <- "win"
  fixations_loss$condition <- "loss"
  
  choices   <- rbind(choices_gain, choices_loss)
  fixations <- rbind(fixations_gain, fixations_loss)
  
  save(choices, file=file.path(outdir,"raw_choices.RData"))
  save(fixations, file=file.path(outdir,"raw_fixations.RData"))

}
```

## Clean

Clean and export the datasets. Store in-sample (odd) and out-of-sample (even) trials separately.

```{r clean}

if (!quickdataclean) {

# FUNCTIONS

clean.choices <- function(choices) {
  choices <- choices %>%
    mutate(
      subject = subject_ID,
      trial = trial_number,
      Condition = factor(
        ifelse(trial_type == "win", 1, 0),
        levels = c(0, 1),
        labels = c("Loss", "Gain")
      ),
      potentialoutcome = ifelse(Condition=="Gain", 10, -10),
      vL = p_left/10*potentialoutcome,
      vR = p_right/10*potentialoutcome,
      vDiff = round(vL-vR, 1),
      difficulty= abs(vDiff),
      choice = ifelse(choice == "left", 1, 0),
      better_option = factor(
        ifelse(vDiff<0, 0, 1),
        levels=c(0,1),
        labels=c("Right","Left")
      ),
      rt = RT
    ) %>%
    group_by(subject, Condition, vDiff) %>%
    mutate(
      choice.corr = choice - mean(choice)
    ) %>%
    subset(select = c(
      subject,
      trial,
      Condition,
      vL,
      vR,
      vDiff,
      difficulty,
      choice,
      choice.corr,
      better_option,
      rt
    ))
  return(choices)
}

clean.fixations <- function(fixations) {
  fixations <- fixations %>%
    mutate(
      subject = subject_ID,
      trial = trial_number,
      Condition = factor(
        ifelse(condition == "win", 1, 0),
        levels = c(0, 1),
        labels = c("Loss", "Gain")
      ),
      Location = factor(
        location, 
        levels=c(1,2), 
        labels=c("Left","Right")
      ),
      fix_dur = fix_dur/1000,
      firstFix = ifelse(fix_num == 1, 1, 0),
    ) %>%
    group_by(subject, trial, condition) %>%
    mutate(
      middleFix = ifelse(fix_num > 1 & fix_num != max(fix_num), 1, 0),
      lastFix = ifelse(fix_num == max(fix_num) & fix_num > 1, 1, 0),
      net_fix = sum( fix_dur * ifelse(Location=="Left",1,-1) )
    ) %>%
    ungroup() %>%
    mutate(
      fix_type = factor(
        1*firstFix + 2*middleFix + 3*lastFix,
        levels = c(1,2,3),
        labels = c("First","Middle","Last")
      )
    ) %>%
    subset(
      select = c(
        subject,
        trial,
        Condition,
        Location,
        net_fix,
        fix_num,
        fix_dur,
        fix_type
      )
    )
  return(fixations)
}

make.cfr <- function(choices, fixations) {
  cfr <- merge(choices, fixations, by=c("subject","trial","Condition"))
  cfr <- cfr[order(cfr$subject,cfr$trial,cfr$Condition,cfr$fix_num),]
  cfr <- cfr %>%
    mutate(
      subject = as.integer(factor(subject))
    ) %>%
    group_by(subject, Condition, trial) %>%
    mutate(
      firstSeenChosen = first(Location)==better_option,
      ndt = rt - sum(fix_dur)
    ) 
  return(cfr)
}


######################
## Exploratory
######################

## load

source("set_e_dir.R")
load(file.path(outdir,"raw_choices.RData"))
load(file.path(outdir,"raw_fixations.RData"))

## clean

choices <- clean.choices(choices)
fixations <- clean.fixations(fixations)

## cfr (cleaned choice data + cleaned fixation data)

cfr <- make.cfr(choices, fixations)
cfr_even <- cfr[cfr$trial%%2==0,]
cfr_odd <- cfr[cfr$trial%%2!=0,]

## save

save(cfr, file=file.path(outdir,"cfr.RData"))
save(cfr_even, file=file.path(outdir,"cfr_even.RData"))
save(cfr_odd, file=file.path(outdir,"cfr_odd.RData"))

######################
## Confirmatory
######################

## load

source("set_c_dir.R")
load(file.path(outdir,"raw_choices.RData"))
load(file.path(outdir,"raw_fixations.RData"))

## clean

choices <- clean.choices(choices)
fixations <- clean.fixations(fixations)

## cfr (cleaned choice data + cleaned fixation data)

cfr <- make.cfr(choices, fixations)
cfr_even <- cfr[cfr$trial%%2==0,]
cfr_odd <- cfr[cfr$trial%%2!=0,]

## save

save(cfr, file=file.path(outdir,"cfr.RData"))
save(cfr_even, file=file.path(outdir,"cfr_even.RData"))
save(cfr_odd, file=file.path(outdir,"cfr_odd.RData"))

}

```

## Useful Functions

A collection of useful functions that will help at one point or another in this project.

```{r usefulFunctions}

# estimate and return the mode of a distribution (in this case, the posterior)
estimate_mode <- function(x) {
  d <- density(x)
  return( d$x[which.max(d$y)] )
}

# estimate 95 HDI
estimate_hdi <- function(x, conf=.95) {
  lb <- 0 + (1-conf)/2
  ub <- 1 - (1-conf)/2
  d <- quantile(x, probs = c(lb,ub), na.rm = F,
           names = TRUE)
  return(d)
}

```


## Plot Options

These are ggplot features that you want to stay constant across all your plots. They serve as a default, but dont worry, you can override them anytime if you add the same argument!

```{r plotOptions}

# Color palette
my_colors = list(
  gain_loss_colors = c("red3", "green4", "blue2", "purple2", "orange2", "deeppink", "cyan3", "bisque3")
)
cvi_palettes = function(name, n, all_palettes = my_colors, type = c("discrete", "continuous")) {
  palette = all_palettes[[name]]
  if (missing(n)) {
    n = length(palette)
  }
  type = match.arg(type)
  out = switch(type,
               continuous = grDevices::colorRampPalette(palette)(n),
               discrete = palette[1:n]
  )
  structure(out, name = name, class = "palette")
}
scale_color_glcolor = function(name) {
  ggplot2::scale_colour_manual(values = cvi_palettes(name,
                                                    type = "discrete"))
}
scale_fill_glcolor = function(name) {
  ggplot2::scale_fill_manual(values = cvi_palettes(name,
                                                    type = "discrete"))
}

# ggplot options
ggplot <- function(...) ggplot2::ggplot(...) + 
  theme_bw() +
  scale_color_glcolor("gain_loss_colors") +
  scale_fill_glcolor("gain_loss_colors") +
  coord_cartesian(expand=FALSE) +
  theme(
    legend.position="None",
    plot.margin = unit(c(.5,.5,.5,.5), "cm"),
    text = element_text(size=18),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  guides(color=guide_legend(override.aes=list(fill=NA)))
  

linesize = 2
markersize = .1
ribbonalpha = 0.33

```

## Regression Options

These are the brms hierarchical regression arguments that you want to stay constant across all regressions. Dont worry, you can always overwrite them on the spot!

```{r regOptions}

if (quickreg==T) {refit="never"} else {refit="on_change"}

brm <- function(...)
  brms::brm(
    ...,
    iter = 6000, #samples from the posterior
    warmup = 3000, #part of a healthy workout
    chains = 3, #parallel
    cores = 3, #parallel
    backend = 'rstan',
    seed = seed, #based gaming
    refresh = show.reg.progress, #0 = dont show updates of sampling progress. 1 = what do you think...?
    file_refit = refit #if quickly is set to false, then re-run the regressions. o/w, pull reg results from temp files.
  ) 

```

--------------------------------------------------------------------------------------------------------

# Psychometrics {.tabset}

## Choice

Probability of choosing left wrt value difference (L-R).

```{r choiceprob, message=FALSE}

## Plot function

psycho.choice.plt <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, vDiff) %>%
    summarize(
      choice.mean = mean(choice) 
    ) %>%
    ungroup() %>%
    group_by(Condition, vDiff) %>%
    summarize(
      y = mean(choice.mean),
      se = std.error(choice.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=vDiff, y=y, group=Condition)) +
    geom_hline(yintercept=0.5, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(-1,1)) +
    ylim(c(0,1)) +
    labs(y="Pr(Choose Left)", x="Left - Right E[V]") +
    theme(
      legend.position=c(0.2,0.85)
    )
    
  
  return(plt)
}

## Regression function

psycho.choice.reg <- function(data) {
  
  # Convert to Binomial data
  data <- data[data$fix_type=="First",]
  data <- data %>% mutate(n=1)
  data <-  data %>%
    group_by(subject, Condition, vDiff) %>%
    summarize(n = sum(n),
              choice = sum(choice))
  
  results <- brm(
    choice | trials(n) ~ vDiff*Condition + (1+vDiff*Condition | subject),
    data = data,
    family = binomial(link='logit'),
    file = file.path(tempdir, "psycho.choice")
  )
  
  return(results)

}

######################
## Exploratory
######################

## load

source("set_e_dir.R")
source("load_data.R")

## Choice probabilities

plt.choice.e <- psycho.choice.plt(cfr)
plt.choice.e
ggsave(filename = file.path(figdir, "psychometric_curve.pdf"), plot = plt.choice.e)
ggsave(filename = file.path(figdir, "psychometric_curve.png"), plot = plt.choice.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.choice.e <- psycho.choice.reg(cfr)
  fixef(reg.choice.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

######################
## Joint
######################

# plt.simulations <- grid.arrange(
#   arrangeGrob(
#     plt.choice.sim.gain + theme(plot.background = element_rect(fill = 'lightcyan', color = 'lightcyan')), 
#     plt.rt.sim.gain + theme(plot.background = element_rect(fill = 'lightcyan', color = 'lightcyan')),
#     nrow=2,
#     top=text_grob("Gain", size=14, face=2)
#   ),
#   arrangeGrob(
#     plt.choice.sim.loss + theme(plot.background = element_rect(fill = 'mistyrose', color = 'mistyrose')), 
#     plt.rt.sim.loss + theme(plot.background = element_rect(fill = 'mistyrose', color = 'mistyrose')),
#     nrow=2,
#     top=text_grob("Loss", size=14, face=2)
#   ),
#   ncol=2
# )

```

When items are valued equally, the probability of choosing the left item is slightly but significantly lower than 50\%.
The probability of choosing left is increasing in the relative value of the left option, and is significantly larger in the Gain condition.

## RT

Response time wrt difficulty.

```{r rt, message=FALSE}

## Plot function

psycho.rt.plt <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(
      rt.mean = mean(rt) 
    ) %>%
    ungroup() %>%
    group_by(Condition, difficulty) %>%
    summarize(
      y = mean(rt.mean),
      se = std.error(rt.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=difficulty, y=y, group=Condition)) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(0,1)) +
    ylim(c(0,4)) +
    labs(y="Response Time (s)", x="Best - Worst E[V]")
    
  
  return(plt)
}

## Regression function

psycho.rt.reg <- function(data) {

  data <- data[data$fix_type=="First",]
    
  results <- brm(
    rt ~ difficulty*Condition + (1+difficulty*Condition | subject),
    data=data,
    family = gaussian(),
    file = file.path(tempdir, "psycho.rt")
  )
  
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.rt.e <- psycho.rt.plt(cfr)
plt.rt.e
ggsave(filename = file.path(figdir, "response_time.pdf"), plot = plt.rt.e)
ggsave(filename = file.path(figdir, "response_time.png"), plot = plt.rt.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.rt.e <- psycho.rt.reg(cfr)
  fixef(reg.rt.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

## Fixations

Number of fixations wrt difficulty.

```{r numfix, message=FALSE}

## Plot function

psycho.numfix.plt <- function(data) {
  
  pdata <- data[data$fix_type=="Last",] %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(
      fix_num.mean = mean(fix_num) 
    ) %>%
    ungroup() %>%
    group_by(Condition, difficulty) %>%
    summarize(
      y = mean(fix_num.mean),
      se = std.error(fix_num.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=difficulty, y=y, group=Condition)) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(0,1)) +
    ylim(c(0,5)) +
    labs(y="Number of Fixations", x="Best - Worst E[V]")
    
  
  return(plt)
  
}

## Regression function

psycho.numfix.reg <- function(data) {

  data <- data[data$fix_type=="Last",]
    
  results <- brm(
    fix_num ~ difficulty*Condition + (1+difficulty*Condition | subject),
    data=data,
    family = gaussian(),
    file = file.path(tempdir, "psycho.numfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.numfix.e <- psycho.numfix.plt(cfr)
plt.numfix.e
ggsave(filename = file.path(figdir, "num_fix.pdf"), plot = plt.numfix.e)
ggsave(filename = file.path(figdir, "num_fix.png"), plot = plt.numfix.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.numfix.e <- psycho.numfix.reg(cfr)
  fixef(reg.numfix.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

In the loss condition, there are roughly 4 fixations per trial on average.
This number decreases slightly with ease of the decision.

--------------------------------------------------------------------------------------------------------

# Fixation Properties {.tabset}

## Pr(1st Best)

Probability of first fixation to best alternative wrt difficulty (excluding 0).

```{r prfirst, message=FALSE}

## Plot function

fixprop.prfirst.plt <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(
      firstbest.mean = mean(Location==better_option) 
    ) %>%
    ungroup() %>%
    group_by(Condition, difficulty) %>%
    summarize(
      y = mean(firstbest.mean),
      se = std.error(firstbest.mean)
    )
  
  plt <- ggplot(data=pdata[pdata$difficulty>0,], aes(x=difficulty, y=y, group=Condition)) +
    geom_hline(yintercept=0.5, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(NA,1)) +
    ylim(c(0,1)) +
    labs(y="Pr(First Fix. to Best)", x="Best - Worst E[V]") +
    theme(
      legend.position = c(0.2,0.85)
    )
    
  return(plt)
  
}

## Regression function

fixprop.prfirst.reg <- function(data) {

  data <- data[data$fix_type=="First",]
  data$firstBest <- data$Location==data$better_option
  data <- data %>% mutate(n=1)
  data <-  data %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(n = sum(n),
              firstBest = sum(firstBest))
    
  results <- brm(
    firstBest | trials(n) ~ difficulty*Condition + (1+difficulty*Condition | subject),
    data=data,
    family = binomial(link="logit"),
    file = file.path(tempdir, "fixprop.prfirst")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.prfirst.e <- fixprop.prfirst.plt(cfr)
plt.prfirst.e
ggsave(filename = file.path(figdir, "prFirstBest.pdf"), plot = plt.prfirst.e)
ggsave(filename = file.path(figdir, "prFirstBest.png"), plot = plt.prfirst.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.prfirst.e <- fixprop.prfirst.reg(cfr)
  fixef(reg.prfirst.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

The probability that the first fixation is to the better option is roughly 50\%.

## Type

Fixation duration by fixation type and condition.

```{r fixtype, message=FALSE}

## Plot function

fixprop.fixtype.plt <- function(data) {
  
  pdata.F <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition) %>%
    summarize(
      fix_dur.mean = mean(fix_dur) 
    ) %>%
    ungroup() %>%
    group_by(Condition) %>%
    summarize(
      y = mean(fix_dur.mean),
      se = std.error(fix_dur.mean),
      x = 1
    )
  pdata.M <- data[data$fix_type=="Middle",] %>%
    group_by(subject, Condition) %>%
    summarize(
      fix_dur.mean = mean(fix_dur) 
    ) %>%
    ungroup() %>%
    group_by(Condition) %>%
    summarize(
      y = mean(fix_dur.mean),
      se = std.error(fix_dur.mean),
      x = 2
    )
  pdata.L <- data[data$fix_type=="Last",] %>%
    group_by(subject, Condition) %>%
    summarize(
      fix_dur.mean = mean(fix_dur) 
    ) %>%
    ungroup() %>%
    group_by(Condition) %>%
    summarize(
      y = mean(fix_dur.mean),
      se = std.error(fix_dur.mean),
      x = 3
    )
  pdata <- bind_rows(pdata.F,pdata.M,pdata.L)
  pdata$x <- factor(pdata$x, levels=c(1,2,3), labels=c("First","Middle","Last"))
  
  plt <- ggplot(data=pdata, aes(x=x, y=y, group=Condition)) +
    geom_bar(aes(fill=Condition), position=position_dodge(.9), stat="identity") +
    geom_errorbar(aes(ymin=y-se, ymax=y+se), position=position_dodge(.9), color='black', width=markersize) +
    ylim(c(0,1)) +
    labs(y="Fixation Duration (s)", x="Fixation Type")
    
  
  return(plt)
}

## Difference t-tests

fixmean.F <- cfr[cfr$fix_type=="First",] %>%
  group_by(subject, Condition) %>%
  summarize(
    fix_dur.mean = mean(fix_dur) 
  ) %>%
  ungroup()
t.test.F <- t.test(
  fixmean.F[fixmean.F$Condition=="Gain",]$fix_dur.mean,
  fixmean.F[fixmean.F$Condition=="Loss",]$fix_dur.mean
)
cohen.d.F <- cohen.d(
  fixmean.F[fixmean.F$Condition=="Gain",]$fix_dur.mean,
  fixmean.F[fixmean.F$Condition=="Loss",]$fix_dur.mean
)
t.test.F
cohen.d.F

fixmean.M <- cfr[cfr$fix_type=="Middle",] %>%
  group_by(subject, Condition) %>%
  summarize(
    fix_dur.mean = mean(fix_dur) 
  ) %>%
  ungroup()
t.test.M <- t.test(
  fixmean.M[fixmean.M$Condition=="Gain",]$fix_dur.mean,
  fixmean.M[fixmean.M$Condition=="Loss",]$fix_dur.mean
)
cohen.d.M <- cohen.d(
  fixmean.M[fixmean.M$Condition=="Gain",]$fix_dur.mean,
  fixmean.M[fixmean.M$Condition=="Loss",]$fix_dur.mean
)
t.test.M
cohen.d.M

fixmean.L <- cfr[cfr$fix_type=="Last",] %>%
  group_by(subject, Condition) %>%
  summarize(
    fix_dur.mean = mean(fix_dur) 
  ) %>%
  ungroup()
t.test.L <- t.test(
  fixmean.L[fixmean.L$Condition=="Gain",]$fix_dur.mean,
  fixmean.L[fixmean.L$Condition=="Loss",]$fix_dur.mean
)
cohen.d.L <- cohen.d(
  fixmean.L[fixmean.L$Condition=="Gain",]$fix_dur.mean,
  fixmean.L[fixmean.L$Condition=="Loss",]$fix_dur.mean
)
t.test.L
cohen.d.L

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.fixtype.e <- fixprop.fixtype.plt(cfr)
plt.fixtype.e
ggsave(filename = file.path(figdir, "fixDurFixType.pdf"), plot = plt.fixtype.e)
ggsave(filename = file.path(figdir, "fixDurFixType.png"), plot = plt.fixtype.e, width=6, height=6*3/4, units="in")


plt.fixtype.e

```

Middle fixations are longest, followed by first fixations, then last fixations.
There is no statistical difference in fixation durations by fixation type across Gain-Loss conditions.

## Middle

Middle fixation duration by difficulty.

```{r middur, message=FALSE}

## Plot function

fixprop.mid.plt <- function(data) {
  
  pdata <- data[data$fix_type=="Middle",] %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(
      mid.mean = mean(fix_dur) 
    ) %>%
    ungroup() %>%
    group_by(Condition, difficulty) %>%
    summarize(
      y = mean(mid.mean),
      se = std.error(mid.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=difficulty, y=y, group=Condition)) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(0,1)) +
    ylim(c(0,1)) +
    labs(y="Middle Fix. Duration (s)", x="Best - Worst E[V]")
    
  
  return(plt)
  
}

## Regression function

fixprop.mid.reg <- function(data) {

  data <- data[data$fix_type=="Middle",]
    
  results <- brm(
    fix_dur ~ difficulty*Condition + (1+difficulty*Condition | subject),
    data=data,
    family = gaussian(),
    prior = c(
      prior(normal(0,800), class=Intercept),
      prior(normal(0,50), class=b)
    ),
    file = file.path(tempdir, "fixprop.mid")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.mid.e <- fixprop.mid.plt(cfr)
plt.mid.e
ggsave(filename = file.path(figdir, "midFixDur.pdf"), plot = plt.mid.e)
ggsave(filename = file.path(figdir, "midFixDur.png"), plot = plt.mid.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.mid.e <- fixprop.mid.reg(cfr)
  fixef(reg.mid.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

Middle fixation durations are on average about 700 ms and decrease with ease of decision.

## First

First fixation duration wrt difficulty.

```{r firstdur, message=FALSE}

## Plot function

fixprop.first.plt <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, difficulty) %>%
    summarize(
      mid.mean = mean(fix_dur) 
    ) %>%
    ungroup() %>%
    group_by(Condition, difficulty) %>%
    summarize(
      y = mean(mid.mean),
      se = std.error(mid.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=difficulty, y=y, group=Condition)) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(0,1)) +
    ylim(c(0,1)) +
    labs(y="First Fix. Duration (s)", x="Best - Worst E[V]")
    
  
  return(plt)
}

## Regression function

fixprop.first.reg <- function(data) {

  data <- data[data$fix_type=="First",]
    
  results <- brm(
    fix_dur ~ difficulty*Condition + (1+difficulty*Condition | subject),
    data=data,
    family = gaussian(),
    prior = c(
      prior(normal(0,700), class=Intercept),
      prior(normal(0,100), class=b)
    ),
    file = file.path(tempdir, "fixprop.first")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.first.e <- fixprop.first.plt(cfr)
plt.first.e
ggsave(filename = file.path(figdir, "firstFixDur.pdf"), plot = plt.first.e)
ggsave(filename = file.path(figdir, "firstFixDur.png"), plot = plt.first.e, width=6, height=6*3/4, units="in")

if(!quickreg) {
  reg.first.e <- fixprop.first.reg(cfr)
  fixef(reg.first.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

First fixations are on average roughly 425 ms.

## Net

Net fixation duration (L-R) wrt value difference (L-R).

```{r netdur, message=FALSE}

## Plot function

fixprop.net.plt <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, vDiff) %>%
    summarize(
      net.mean = mean(net_fix) 
    ) %>%
    ungroup() %>%
    group_by(Condition, vDiff) %>%
    summarize(
      y = mean(net.mean),
      se = std.error(net.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=vDiff, y=y, group=Condition)) +
    geom_hline(yintercept=0, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(-1,1)) +
    ylim(c(-.5,.5)) +
    labs(y="Net Fix. Duration (L-R, s)", x="Left - Right E[V]")
    
  
  return(plt)
}

## Regression function

fixprop.net.reg <- function(data) {

  data <- data[data$fix_type=="First",]
    
  results <- brm(
    net_fix ~ vDiff*Condition + (1+vDiff*Condition | subject),
    data=data,
    family = gaussian(),
    prior = c(
      prior(normal(0,1000), class=b)
    ),
    file = file.path(tempdir, "fixprop.net")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.net.e <- fixprop.net.plt(cfr)
plt.net.e
ggsave(filename = file.path(figdir, "netFixNetVal.pdf"), plot = plt.net.e)
ggsave(filename = file.path(figdir, "netFixNetVal.png"), plot = plt.net.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.net.e <- fixprop.net.reg(cfr)
  fixef(reg.net.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

Subjects spend slightly more time looking at the option with the higher expected value.

--------------------------------------------------------------------------------------------------------

# Choice Biases {.tabset}

## Last

Last fixation bias.

Probability of choosing left wrt value difference (L-R), separated by location of last fixation.

```{r lastfixbias, message=FALSE, collapse=FALSE}

## Plot function

bias.lastfix.plt <- function(data) {
  
  pdata <- data[data$fix_type=="Last",] %>%
    group_by(subject, Condition, Location, vDiff) %>%
    summarize(
      choice.mean = mean(choice) 
    ) %>%
    ungroup() %>%
    group_by(Condition, Location, vDiff) %>%
    summarize(
      y = mean(choice.mean),
      se = std.error(choice.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=vDiff, y=y, linetype=Location)) +
    geom_hline(yintercept=0.5, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    xlim(c(-1,1)) +
    ylim(c(0,1)) +
    labs(y="Pr(Choose Left)", x="Left - Right E[V]") +
    theme(
      legend.position=c(0.12,0.70),
      legend.key = element_blank(),
      legend.background=element_blank()
    ) +
    guides(
      linetype = guide_legend(override.aes = list(fill = c(NA))),
      color = guide_legend(override.aes=list(fill=NA))
    )
    
  
  return(plt)
}

## Regression function

bias.lastfix.reg <- function(data) {

  data <- data[data$fix_type=="Last",]
  data <- data %>% mutate(n=1)
  data <-  data %>%
    group_by(subject, Condition, Location, vDiff) %>%
    summarize(n = sum(n),
              choice = sum(choice))
    
  results <- brm(
    choice | trials(n) ~ vDiff*Condition*Location + (1+vDiff*Condition*Location | subject),
    data=data,
    family = binomial(link="logit"),
    file = file.path(tempdir, "bias.lastfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.lastfix.e <- bias.lastfix.plt(cfr)
plt.lastfix.e
ggsave(filename = file.path(figdir, "lastFixBias.pdf"), plot = plt.lastfix.e)
ggsave(filename = file.path(figdir, "lastFixBias.png"), plot = plt.lastfix.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.lastfix.e <- bias.lastfix.reg(cfr)
  fixef(reg.lastfix.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

Choices exhibit last fixation bias.
The magnitude of this bias does not change across Gain-Loss conditions.

## Net

Corrected probability of choosing left wrt net fixation left.

```{r netfixbias}
#, message=FALSE, collapse=FALSE}

bias.netfix.plt <- function(data) {
  
  breaks <- seq(-1050,1050,100)/1000
  labels <- seq(-1000,1000,100)/1000
  print(breaks)
  data$net_fix <- cut(data$net_fix, breaks=breaks, labels=labels) %>%
    as.character() %>%
    as.numeric()
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, net_fix) %>%
    summarize(
      choice.mean = mean(choice.corr)
    ) %>%
    ungroup() %>%
    group_by(Condition, net_fix) %>%
    summarize(
      y = mean(choice.mean),
      se = std.error(choice.mean)
    )

  
  plt <- ggplot(data=pdata, aes(x=net_fix, y=y, group=Condition)) +
    geom_hline(yintercept=0, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    labs(y="Corr. Pr(Choose Left)", x="Net Fixation (L-R, s)") +
    xlim(c(-1,1)) +
    ylim(c(-0.4,0.4))
  
  return(plt)
  
}

## Regression function

bias.netfix.reg <- function(data) {

  data <- data[data$fix_type=="First",]
    
  results <- brm(
    choice.corr ~ net_fix*Condition + (1+net_fix*Condition | subject),
    data=data,
    family = gaussian(),
    file = file.path(tempdir, "bias.netfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.netfix.e <- bias.netfix.plt(cfr)
plt.netfix.e
ggsave(filename = file.path(figdir, "netFixBias.pdf"), plot = plt.netfix.e)
ggsave(filename = file.path(figdir, "netFixBias.png"), plot = plt.netfix.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.netfix.e <- bias.netfix.reg(cfr)  
  fixef(reg.netfix.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

The likelihood of choosing an option is increasing in the net fixation to that option.
This likelihood is corrected for the effect of value on choice by subtracting from the probability the average probability of choosing the left option given the net value.

## First

Corrected probability of choosing first seen wrt first fixation duration.

```{r firstfixbias, message=FALSE, collapse=FALSE}

bias.firstfix.plt <- function(data) {
  
  breaks <- seq(-50,1250,100)/1000
  labels <- seq(0,1200,100)/1000
  data$fix_dur <- cut(data$fix_dur, breaks=breaks, labels=labels) %>%
    as.character() %>%
    as.numeric()
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, vDiff) %>%
    mutate(
      firstSeenChosen.corr = firstSeenChosen - mean(firstSeenChosen),
    ) %>%
    ungroup() %>%
    group_by(subject, Condition, fix_dur) %>%
    summarize(
      corrFirst.mean = mean(firstSeenChosen.corr)
    ) %>%
    ungroup() %>%
    group_by(Condition, fix_dur) %>%
    summarize(
      y = mean(corrFirst.mean),
      se = std.error(corrFirst.mean)
    )
  
  plt <- ggplot(data=pdata, aes(x=fix_dur, y=y, group=Condition)) +
    geom_hline(yintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize) +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha) +
    labs(y="Corr. Pr(First Seen Chosen)", x="First Fixation Duration (s)")+
    xlim(c(0,1.2)) +
    ylim(c(-0.5,0.4))
    
  
  return(plt)
}

## Regression function

bias.firstfix.reg <- function(data) {

  data <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, vDiff) %>%
    mutate(
      firstSeenChosen.corr = firstSeenChosen - mean(firstSeenChosen),
    ) 
    
  results <- brm(
    firstSeenChosen.corr ~ fix_dur*Condition + (1+fix_dur*Condition | subject),
    data=data,
    family = gaussian(),
    file = file.path(tempdir, "bias.firstfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.firstfix.e <- bias.firstfix.plt(cfr)
plt.firstfix.e
ggsave(filename = file.path(figdir, "firstFixBias.pdf"), plot = plt.firstfix.e)
ggsave(filename = file.path(figdir, "firstFixBias.png"), plot = plt.firstfix.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.firstfix.e <- bias.firstfix.reg(cfr)
  fixef(reg.firstfix.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

## Last, but just gain condition

Last fixation bias.

Probability of choosing left wrt value difference (L-R), separated by location of last fixation.

```{r lastfixbias, message=FALSE, collapse=FALSE}

## Plot function

bias.lastfixgain.plt <- function(data) {
  
  pdata <- data[data$fix_type=="Last",] %>%
    group_by(subject, Condition, Location, vDiff) %>%
    summarize(
      choice.mean = mean(choice) 
    ) %>%
    ungroup() %>%
    group_by(Condition, Location, vDiff) %>%
    summarize(
      y = mean(choice.mean),
      se = std.error(choice.mean)
    )
  
  plt <- ggplot(data=pdata[pdata$Condition=="Gain",], aes(x=vDiff, y=y, linetype=Location)) +
    geom_hline(yintercept=0.5, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize, color="green4") +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha, fill="green4") +
    xlim(c(-1,1)) +
    ylim(c(0,1)) +
    labs(y="Pr(Choose Left)", x="Left - Right E[V]") +
    theme(
      legend.position=c(0.12,0.70),
      legend.key = element_blank(),
      legend.background=element_blank()
    ) +
    guides(
      linetype = guide_legend(override.aes = list(fill = c(NA))),
      color = guide_legend(override.aes=list(fill=NA))
    )
    
  
  return(plt)
}

## Regression function

bias.lastfixgain.reg <- function(data) {

  data <- data[data$fix_type=="Last",]
  data <- data %>% mutate(n=1)
  data <-  data %>%
    group_by(subject, Condition, Location, vDiff) %>%
    summarize(n = sum(n),
              choice = sum(choice))
    
  results <- brm(
    choice | trials(n) ~ vDiff*Condition*Location + (1+vDiff*Condition*Location | subject),
    data=data,
    family = binomial(link="logit"),
    file = file.path(tempdir, "bias.lastfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.lastfixgain.e <- bias.lastfixgain.plt(cfr)
plt.lastfixgain.e
ggsave(filename = file.path(figdir, "lastFixBias_GainOnly.pdf"), plot = plt.lastfixgain.e)
ggsave(filename = file.path(figdir, "lastFixBias_GainOnly.png"), plot = plt.lastfixgain.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.lastfixgain.e <- bias.lastfixgain.reg(cfr)
  fixef(reg.lastfixgain.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

## Net, but just gain

Corrected probability of choosing left wrt net fixation left.

```{r netfixbias}
#, message=FALSE, collapse=FALSE}

bias.netfixgain.plt <- function(data) {
  
  breaks <- seq(-1050,1050,100)/1000
  labels <- seq(-1000,1000,100)/1000
  print(breaks)
  data$net_fix <- cut(data$net_fix, breaks=breaks, labels=labels) %>%
    as.character() %>%
    as.numeric()
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition, net_fix) %>%
    summarize(
      choice.mean = mean(choice.corr)
    ) %>%
    ungroup() %>%
    group_by(Condition, net_fix) %>%
    summarize(
      y = mean(choice.mean),
      se = std.error(choice.mean)
    )

  
  plt <- ggplot(data=pdata[pdata$Condition=="Gain", ], aes(x=net_fix, y=y, group=Condition)) +
    geom_hline(yintercept=0, color="grey", alpha=0.75) +
    geom_vline(xintercept=0, color="grey", alpha=0.75) +
    geom_line(aes(color=Condition), size=linesize, color="green4") +
    geom_ribbon(aes(ymin=y-se, ymax=y+se, fill=Condition), alpha=ribbonalpha, fill="green4") +
    labs(y="Corr. Pr(Choose Left)", x="Net Fixation (L-R, s)") +
    xlim(c(-1,1)) +
    ylim(c(-0.4,0.4))
  
  return(plt)
  
}

## Regression function

bias.netfixgain.reg <- function(data) {

  data <- data[data$fix_type=="First",]
    
  results <- brm(
    choice.corr ~ net_fix*Condition + (1+net_fix*Condition | subject),
    data=data,
    family = gaussian(),
    file = file.path(tempdir, "bias.netfix")
  )
  return(results)

}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.netfixgain.e <- bias.netfixgain.plt(cfr)
plt.netfixgain.e
ggsave(filename = file.path(figdir, "netFixBias_GainOnly.pdf"), plot = plt.netfixgain.e)
ggsave(filename = file.path(figdir, "netFixBias_GainOnly.png"), plot = plt.netfixgain.e, width=6, height=6*3/4, units="in")

if (!quickreg) {
  reg.netfixgain.e <- bias.netfixgain.reg(cfr)  
  fixef(reg.netfixgain.e)[,c('Estimate', 'Q2.5', 'Q97.5')]
}

```

Need to rerun the regressions.
Seems that the corrected probability of choosing the first seen option is slightly increasing in first fixation duration, but the effect might not be significant.

--------------------------------------------------------------------------------------------------------

# Model Fitting {.tabset}

## DAG

The directed acyclic graph for the aDDM. I figured how to take out trial-by-trial drift rate variability from the Lombardi-Hare toolbox. Now it's a standard aDDM!

![](../BE_writeups/fig_hierarchical_model.png)

## JAGS

(1) The JAGS model for fitting the aDDM, and (2) Initial guesses for the MCMC algorithm.

* I let theta be between -0.5 and 3. 
* I've taken out trial-by-trial noise in the drift rate parameter.

```{r jagsmodel}

########################
## MODEL
########################

cat( 
"
model {
  
  # drift rate
      d_mu ~ dunif(0.00001,10)
      d_pr ~ dgamma(1, 0.1)
  
  # noise
      sig_mu ~ dunif(0.000001, 2)
      sig_pr ~ dgamma(1, 0.1)
  
  # Bias of the DDM
      bias_alpha <- bias_mu * bias_kappa
      bias_beta <- (1 - bias_mu) * bias_kappa
      bias_mu ~ dbeta(2, 2)T(0.01,0.99)
      bias_kappa ~ dgamma(1, 0.5)
  
  # attentional bias
      theta_mu ~ dnorm(0,.5)T(-0.5,3)
      theta_pr ~ dgamma(1, 0.1)
  
  for (p in 1:ns) { # subject level
  
      d[p] ~ dnorm(d_mu, d_pr)T(0.000001,10)
      
      sig[p] ~ dnorm(sig_mu, sig_pr)T(0.00001,2)
      
      bias[p] ~ dbeta(bias_alpha, bias_beta)T(0.01,0.99)
  
      theta[p] ~ dnorm(theta_mu, theta_pr)T(-0.5,3)
  }
  
  for (i in 1:N) { # trial level
  
  ## WIENER model, fixing the threshold to 2 and estimating the noise
      y[i] ~ dwieners(2, tau[i], bet[i], w[i], sig[idxP[i]] ) # actual DDM distribution
      
      # generate predictions
      y_hat[i] ~ dwieners(2, tau[i], bet[i], w[i], sig[idxP[i]] ) # in-sample predictions
  
      # generate trial-by-trial nDT
      tau[i] <- ndt[i]
  
      # generate trial-by-trial Bias
      bet[i] <- bias[idxP[i]]
  
      # Drift rate
      w[i] <- d[idxP[i]] * ( (gazeL[i]*v_L[i] - (1-gazeL[i])*v_R[i]) + theta[idxP[i]]*((1-gazeL[i])*v_L[i] - gazeL[i]*v_R[i]))
  
  }
}
" ,
file=file.path("JAGS-aDDM.txt") )


########################
## INITIAL GUESSES
########################

inits1 <- dump.format(list( 
  d_mu=0.5, d_pr=0.5,
  sig_mu=0.1,sig_pr=1,
  bias_mu=0.45, bias_kappa=0.9,
  theta_mu=-0.5,theta_pr=0.9,
  .RNG.name="base::Super-Duper", .RNG.seed=1
))

inits2 <- dump.format(list( 
  d_mu=1, d_pr=0.4,
  sig_mu=0.5,sig_pr=1,
  bias_mu=0.5, bias_kappa=1,
  theta_mu=0.5,theta_pr=1,
  .RNG.name="base::Wichmann-Hill", .RNG.seed=2
))

inits3 <- dump.format(list( 
  d_mu=1.5, d_pr=0.6,
  sig_mu=1,sig_pr=1,
  bias_mu=0.55, bias_kappa=1.1,
  theta_mu=1.5,theta_pr=1.1,
  .RNG.name="base::Mersenne-Twister", .RNG.seed=3
))


###############################
## WHICH PARAMETERS TO MONITOR?
###############################

monitor = c(
  "d_mu", "d_pr", "d", # drift rate
  "sig_mu", "sig_pr", "sig", # noise 
  "bias_mu", "bias_kappa", "bias", # bias
  "theta_mu", "theta_pr", "theta", # attentional discounting
  "y_hat" # predictions
)

###############################
## EVERYTHING ELSE ALL AT ONCE
###############################

chainscores = 3
iterations = 10000 # x chainscores = total posterior samples
warmup = 30000

```

## Lombari-Hare Toolbox

Code modified from Lombardi & Hare (2021) toolbox. This is going to take a version of the cfr dataframe and convert it into a dataset that JAGS can use to fit the aDDM.

```{r lhtoobox}

# # # Input a dataframe with: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#
# choice   -> =1 if left food item chosen, =0 if right food item chosen
# leftval  -> value option on the left (scale this in code below)
# rightval -> value option on the right
# rt       -> reaction time in s
# fixnum   -> fixation number
# fixdur   -> fixation duration in s
# roi      -> region of interest of the fixation (1 if fixation to the left option, 0 to the right option)
# trial    -> trial number
# subject  -> subject number
#
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

LHToolbox <- function(data) {
  
  # subject numbers
  subjs<- unique(data$subject)
  
  # calculate total fixation duration
  # total fixation time to the left option 
  # and total fixation time to the right option
  for (s in subjs) {
    for (t in unique(data$trial[data$subject==s])) {
      #index for all fixations in one trial
      ind=which(data$subject==s & data$trial==t)
      #index for left fixations in the trial
      indl=which(data$subject==s & data$trial==t & data$roi==1)
      #index for right fixations in the trial
      indr=which(data$subject==s & data$trial==t & data$roi==0)
      #total fixation duration
      data$totfix[data$subject==s & data$trial==t] = sum(data$fixdur[ind])
      #fixation time to the left option
      data$fixleft[data$subject==s & data$trial==t] = sum(data$fixdur[indl])
      #fixation time to the right option
      data$fixright[data$subject==s & data$trial==t] = sum(data$fixdur[indr])
    }
  }
  
  # discard all the fixations, keep the first one
  data<-data[data$fixnum==1,]
  
  #non decision time = rt - total fixation time
  data$ndt <- (data$rt - data$totfix) # in seconds
  #data$ndt[data$ndt<0] <- 0.0001 # you can decide whether to fit the ndt or give it as input to the model
  
  # NB BEFORE FITTING THE MODEL MAKE SURE YOU HAVE NO NAN or NA IN YOUR DATA
  
  #--------------------------------#--------------------------------
  
  # RT is positive if left food item chosen, negative if right food item chosen
  data$rtPN[data$choice==1]<- data$rt[data$choice==1] # in seconds
  data$rtPN[data$choice==0]<- -data$rt[data$choice==0] # in seconds
  
  # Index of subjects
  idxP = as.numeric(ordered(data$subject)) #makes a sequentially numbered subj index
  
  # rescale the value of the options
  v_left = data$leftval/5.5 # scaled by the maximum value left or right can be: 5.5
  v_right = data$rightval/5.5
  
  # value diff
  vDiff = round(v_left-v_right,1)
  
  # proportion of fixations to the left option (nb. fixright = 1-gazeL)
  gazeL = data$fixleft/data$totfix

  # rt to fit
  y = data$rtPN
  
  # number of trials
  N = length(y)
  
  # number of subjects
  ns = length(unique(idxP))
  
  # non-decision time
  ndt = data$ndt
  
  #--------------------------------------------
  # fit the model
  
  # data
  dat <- dump.format(list(
    N=N, 
    y=y, 
    idxP=idxP, 
    v_L=v_left,
    v_R=v_right, 
    gazeL=gazeL, 
    ns=ns, 
    ndt=ndt
  ))
  
  return(dat)
}

```

Get y

```{r get_y}

# # # Input a dataframe with: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
#
# choice   -> =1 if left food item chosen, =0 if right food item chosen
# leftval  -> value option on the left (scale this in code below)
# rightval -> value option on the right
# rt       -> reaction time in s
# fixnum   -> fixation number
# fixdur   -> fixation duration in s
# roi      -> region of interest of the fixation (1 if fixation to the left option, 0 to the right option)
# trial    -> trial number
# subject  -> subject number
#
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

getY <- function(data) {
  
  # subject numbers
  subjs<- unique(data$subject)
  
  # calculate total fixation duration
  # total fixation time to the left option 
  # and total fixation time to the right option
  for (s in subjs) {
    for (t in unique(data$trial[data$subject==s])) {
      #index for all fixations in one trial
      ind=which(data$subject==s & data$trial==t)
      #index for left fixations in the trial
      indl=which(data$subject==s & data$trial==t & data$roi==1)
      #index for right fixations in the trial
      indr=which(data$subject==s & data$trial==t & data$roi==0)
      #total fixation duration
      data$totfix[data$subject==s & data$trial==t] = sum(data$fixdur[ind])
      #fixation time to the left option
      data$fixleft[data$subject==s & data$trial==t] = sum(data$fixdur[indl])
      #fixation time to the right option
      data$fixright[data$subject==s & data$trial==t] = sum(data$fixdur[indr])
    }
  }
  
  # discard all the fixations, keep the first one
  data<-data[data$fixnum==1,]
  
  #non decision time = rt - total fixation time
  data$ndt <- (data$rt - data$totfix) # in seconds
  #data$ndt[data$ndt<0] <- 0.0001 # you can decide whether to fit the ndt or give it as input to the model
  
  # NB BEFORE FITTING THE MODEL MAKE SURE YOU HAVE NO NAN or NA IN YOUR DATA
  
  #--------------------------------#--------------------------------
  
  # RT is positive if left food item chosen, negative if right food item chosen
  data$rtPN[data$choice==1]<- data$rt[data$choice==1] # in seconds
  data$rtPN[data$choice==0]<- -data$rt[data$choice==0] # in seconds

  # rt to fit
  y = data$rtPN
  
  return(y)
}

```

## Fit (Gain)

Fit the aDDM in the gain condition. Quickly == TRUE skips the fitting procedure and pulls up old results.

Gelman-Rubin statistic:

```{r addmfitgain, cache=TRUE}

######################
## Exploratory
######################

## Prepare the data for fitting the aDDM

source("set_e_dir.R")
source("load_data.R")

temp <- cfr_odd[cfr_odd$Condition=="Gain",]
temp.gain <- data.frame(
  choice = temp$choice,
  leftval = temp$vL,
  rightval = temp$vR,
  rt = temp$rt,
  fixnum = temp$fix_num,
  fixdur = temp$fix_dur,
  roi = ifelse(temp$Location=="Left",1,0),
  trial = temp$trial,
  subject = temp$subject
)
  
if (quickmodel==FALSE) {
  jagsdata.gain <- LHToolbox(temp.gain)
  
  
  ## Fit the aDDM
  
  set.seed(seed)
  results <- run.jags(
    model="JAGS-aDDM.txt", 
    monitor=monitor, 
    data=jagsdata.gain, 
    n.chains=chainscores, 
    inits=list(inits1,inits2,inits3),
    plots = TRUE, 
    method="parallel", 
    module="wiener", 
    burnin=warmup, sample=iterations
  )
  fit.gain <- add.summary(results)
  save(fit.gain, file=file.path(tempdir, "aDDM-fit-gain.RData"))
} else {
  load(file.path(tempdir, "aDDM-fit-gain.RData"))
}

# Gelman-Rubin statistic. Convergence? King thumb says I'm gucci with anything less than 1.05.
fit.gain$psrf$mpsrf

```

Multiple variable Gelman-Rubin statistic is <1.01 indicating good convergence.

## Fit (Loss)

Fit the aDDM in the loss condition. Quickly == TRUE skips the fitting procedure and pulls up old results.

Gelman-Rubin statistic:

```{r addmfitloss, cache=TRUE}

######################
## Exploratory
######################

## Prepare the data for fitting the aDDM

source("set_e_dir.R")
source("load_data.R")

temp <- cfr_odd[cfr_odd$Condition=="Loss",]
temp.loss <- data.frame(
  choice = temp$choice,
  leftval = temp$vL,
  rightval = temp$vR,
  rt = temp$rt,
  fixnum = temp$fix_num,
  fixdur = temp$fix_dur,
  roi = ifelse(temp$Location=="Left",1,0),
  trial = temp$trial,
  subject = temp$subject
)
  
if (quickmodel==FALSE) {
  jagsdata.loss <- LHToolbox(temp.loss)
  
  
  ## Fit the aDDM
  
  set.seed(seed)
  results <- run.jags(
    model="JAGS-aDDM.txt", 
    monitor=monitor, 
    data=jagsdata.loss, 
    n.chains=chainscores, 
    inits=list(inits1,inits2,inits3),
    plots = TRUE, 
    method="parallel", 
    module="wiener", 
    burnin=warmup, sample=iterations
  )
  fit.loss <- add.summary(results)
  save(fit.loss, file=file.path(tempdir, "aDDM-fit-loss.RData"))
} else {
  load(file.path(tempdir, "aDDM-fit-loss.RData"))
}

# Gelman-Rubin statistic. Convergence? King thumb says I'm gucci with anything less than 1.05.
fit.loss$psrf$mpsrf

```

Multiple variable Gelman-Rubin statistic is <1.02 indicating good convergence.

## Posterior Predictive Checks

Gain

```{r}

groupvar = c(
  "d_mu", "d_pr", # drift rate
  "sig_mu", "sig_pr", # noise 
  "bias_mu", "bias_kappa", # bias
  "theta_mu", "theta_pr" # attentional discounting
)

fit.gain.mcmc <- as.mcmc.list(fit.gain)
fit.gain.mcmc2 <- as.mcmc(fit.gain)

for (gv in groupvar) {
  diagMCMC(
    fit.gain.mcmc,
    par = gv
  )
}

y <- getY(temp.gain)
yhat <- fit.gain.mcmc2[, grep("y_hat", colnames(fit.gain.mcmc2))]

ppc.gain <- ppc_dens_overlay(
  y,
  yhat[sample(1:30000, 100),]
) 

bayesplot_grid(
  ppc.gain,
  xlim = c(-20,20)
)

```

Loss

```{r}

groupvar = c(
  "d_mu", "d_pr", # drift rate
  "sig_mu", "sig_pr", # noise 
  "bias_mu", "bias_kappa", # bias
  "theta_mu", "theta_pr" # attentional discounting
)

fit.loss.mcmc <- as.mcmc.list(fit.loss)
fit.loss.mcmc2 <- as.mcmc(fit.loss)

for (gv in groupvar) {
  diagMCMC(
    fit.loss.mcmc,
    par = gv
  )
}

y <- getY(temp.loss)
yhat <- fit.loss.mcmc2[, grep("y_hat", colnames(fit.loss.mcmc2))]

ppc.loss <- ppc_dens_overlay(
  y,
  yhat[sample(1:30000, 100),]
) 

bayesplot_grid(
  ppc.loss,
  xlim = c(-20,20)
)

```

--------------------------------------------------------------------------------------------------------

# aDDM {.tabset}

## Individual Estimates

Let's compare individual-level MAP estimates. Only showing the first few subjects.

```{r indivmap, cache=TRUE}

######################
## Exploratory
######################

## load

source("set_e_dir.R")
load(file.path(tempdir, "aDDM-fit-gain.RData"))
load(file.path(tempdir, "aDDM-fit-loss.RData"))
trace.gain <- data.frame(combine.mcmc(fit.gain$mcmc))
trace.loss <- data.frame(combine.mcmc(fit.loss$mcmc))

## Get MAP estimates and their HDIs

MAP.indiv <- data.frame(
  drift.gain         = unique(cfr$subject),
  drift.gain.hdi     = unique(cfr$subject),
  drift.loss         = unique(cfr$subject),
  drift.loss.hdi     = unique(cfr$subject),
  sig.gain       = unique(cfr$subject),
  sig.gain.hdi   = unique(cfr$subject),
  sig.loss       = unique(cfr$subject),
  sig.loss.hdi   = unique(cfr$subject),
  bias.gain      = unique(cfr$subject),
  bias.gain.hdi  = unique(cfr$subject),
  bias.loss      = unique(cfr$subject),
  bias.loss.hdi  = unique(cfr$subject),
  theta.gain     = unique(cfr$subject),
  theta.gain.hdi = unique(cfr$subject),
  theta.loss     = unique(cfr$subject),
  theta.loss.hdi = unique(cfr$subject)
)

for (j in sort(unique(cfr$subject))) {
  
  # Round the estimates
  
  rnd <- 2
  
  # column names
  
  d.ind <- paste0('d.',toString(j),'.')
  sig.ind <- paste0('sig.',toString(j),'.')
  bias.ind <- paste0('bias.',toString(j),'.')
  theta.ind <- paste0('theta.',toString(j),'.')
  
  # Gain: Mode and HDI
  
  MAP.indiv$drift.gain[j]     <- estimate_mode(trace.gain[,d.ind]) %>% round(rnd)
  MAP.indiv$sig.gain[j]   <- estimate_mode(trace.gain[,sig.ind]) %>% round(rnd)
  MAP.indiv$bias.gain[j]  <- estimate_mode(trace.gain[,bias.ind]*2-1) %>% round(rnd) #convert to (-1,1) scale
  MAP.indiv$theta.gain[j] <- estimate_mode(trace.gain[,theta.ind]) %>% round(rnd)
  MAP.indiv$drift.gain.hdi[j]     <- estimate_hdi(trace.gain[,d.ind]) %>% round(rnd) %>% toString()
  MAP.indiv$sig.gain.hdi[j]   <- estimate_hdi(trace.gain[,sig.ind]) %>% round(rnd) %>% toString()
  MAP.indiv$bias.gain.hdi[j]  <- estimate_hdi(trace.gain[,bias.ind]*2-1) %>% round(rnd) %>% toString()
  MAP.indiv$theta.gain.hdi[j] <- estimate_hdi(trace.gain[,theta.ind]) %>% round(rnd) %>% toString()
  
  # Loss: Mode and HDI
  
  MAP.indiv$drift.loss[j]     <- estimate_mode(trace.loss[,d.ind]) %>% round(rnd)
  MAP.indiv$sig.loss[j]   <- estimate_mode(trace.loss[,sig.ind]) %>% round(rnd)
  MAP.indiv$bias.loss[j]  <- estimate_mode(trace.loss[,bias.ind]*2-1) %>% round(rnd)
  MAP.indiv$theta.loss[j] <- estimate_mode(trace.loss[,theta.ind]) %>% round(rnd)
  MAP.indiv$drift.loss.hdi[j]     <- estimate_hdi(trace.loss[,d.ind]) %>% round(rnd) %>% toString()
  MAP.indiv$sig.loss.hdi[j]   <- estimate_hdi(trace.loss[,sig.ind]) %>% round(rnd) %>% toString()
  MAP.indiv$bias.loss.hdi[j]  <- estimate_hdi(trace.loss[,bias.ind]*2-1) %>% round(rnd) %>% toString()
  MAP.indiv$theta.loss.hdi[j] <- estimate_hdi(trace.loss[,theta.ind]) %>% round(rnd) %>% toString()
  
}

# Save and display

save(MAP.indiv, file=file.path(outdir,"MAP-indiv.RData"))
head(MAP.indiv[, grepl("drift",names(MAP.indiv))])
head(MAP.indiv[, grepl("sig",names(MAP.indiv))])
head(MAP.indiv[, grepl("bias",names(MAP.indiv))])
head(MAP.indiv[, grepl("theta",names(MAP.indiv))])

```

## Compare Individuals

Compare the distributions of individual MAP estimates.

```{r compareindiv, warning=FALSE, message=FALSE}

######################
## Exploratory
######################

## load

#source("set_e_dir.R")
#load(file.path(tempdir, "aDDM-fit-gain.RData"))
#load(file.path(tempdir, "aDDM-fit-loss.RData"))
#load(file=file.path(outdir,"MAP-indiv.RData"))
load(file="D:/OneDrive - California Institute of Technology/PhD/Rangel Lab/OLD/2022-gain-loss-attention/00 exploratory/output/MAP-indiv.RData")

## plot

# # graph options # # # #
gradient_resolution = 100
exact = 'grey40'
close = 'grey70'
far = 'white'
# # # # # # # # # # # # #

coord.lim <- 7
d_gradient <- expand.grid(x=seq(0,coord.lim,coord.lim/gradient_resolution), y=seq(0,coord.lim,coord.lim/gradient_resolution))
plt.compare.d.e <- ggplot(data=MAP.indiv) +
  geom_tile(data=d_gradient, aes(x=x, y=y, fill=abs(y-x))) + #add gradient background
  scale_fill_gradient(low=close, high=far) +
  geom_abline(intercept=0, slope=1, color=exact) +
  geom_point(aes(x=drift.gain, y=drift.loss), size=linesize) +
  xlim(c(0,coord.lim)) +
  ylim(c(0,coord.lim)) +
  labs(x = TeX(r"(Gain $d$)"), y = TeX(r"(Loss $d$)")) +
  scale_y_continuous(labels = function(x) format(x, nsmall = 2)) +
  scale_x_continuous(labels = function(x) format(x, nsmall = 2)) 

coord.lim <- 1.2
sig_gradient <- expand.grid(x=seq(0,coord.lim,coord.lim/gradient_resolution), y=seq(0,coord.lim,coord.lim/gradient_resolution))
plt.compare.sig.e <- ggplot(data=MAP.indiv) +
  geom_tile(data=sig_gradient, aes(x=x, y=y, fill=abs(y-x))) +
  scale_fill_gradient(low=close, high=far) +
  geom_abline(intercept=0, slope=1, color=exact) +
  geom_point(aes(x=sig.gain, y=sig.loss), size=linesize) +
  xlim(c(0,coord.lim)) +
  ylim(c(0,coord.lim)) +
  labs(x = TeX(r"(Gain $\sigma$)"), y = TeX(r"(Loss $\sigma$)")) +
  scale_y_continuous(labels = function(x) format(x, nsmall = 2)) +
  scale_x_continuous(labels = function(x) format(x, nsmall = 2)) 

coord.lim <- .4
bias_gradient <- expand.grid(x=seq(-coord.lim,coord.lim,coord.lim/gradient_resolution), y=seq(-coord.lim,coord.lim,coord.lim/gradient_resolution))
plt.compare.bias.e <- ggplot(data=MAP.indiv) +
  geom_tile(data=bias_gradient, aes(x=x, y=y, fill=abs(y-x))) +
  scale_fill_gradient(low=close, high=far) +
  geom_abline(intercept=0, slope=1, color=exact) +
  geom_point(aes(x=bias.gain, y=bias.loss), size=linesize) +
  xlim(c(-coord.lim,coord.lim)) +
  ylim(c(-coord.lim,coord.lim)) +
  labs(x = TeX(r"(Gain bias)"), y = TeX(r"(Loss bias)")) +
  scale_y_continuous(labels = function(x) format(x, nsmall = 2)) +
  scale_x_continuous(labels = function(x) format(x, nsmall = 2)) 

coord.lim <- 3
theta_gradient <- expand.grid(x=seq(0,coord.lim,coord.lim/gradient_resolution), y=seq(0,coord.lim,coord.lim/gradient_resolution))
plt.compare.theta.e <- ggplot(data=MAP.indiv) +
  geom_tile(data=theta_gradient, aes(x=x, y=y, fill=abs(y-x))) +
  scale_fill_gradient(low='orange', high=far) +
  geom_vline(xintercept = 1, color='grey30', alpha=.5) +
  geom_hline(yintercept = 1, color='grey30', alpha=.5) +
  #geom_abline(intercept=0, slope=1, color='grey30') +
  geom_point(aes(x=theta.gain, y=theta.loss), size=linesize) +
  xlim(c(0,coord.lim)) +
  ylim(c(0,coord.lim)) +
  labs(x = TeX(r"(Gain $\theta$)"), y = TeX(r"(Loss $\theta$)")) +
  scale_y_continuous(labels = function(x) format(x, nsmall = 2)) +
  scale_x_continuous(labels = function(x) format(x, nsmall = 2)) 

plt.compare.param.e <- grid.arrange(plt.compare.d.e, plt.compare.sig.e, plt.compare.bias.e, plt.compare.theta.e,
                                    nrow=2)

```

Something odd going on with drift rates.
Probably has something to do with how poorly the aDDM is predicting the out-of-sample data.
On the other hand, we are clearly seeing theta flip over 1 when switching between the gain and loss domains.

## Group Estimates

Let's compare group-level MAP estimates. 95% HDI's below the MAP estimate. They results are displayed in final table format. NA's are just there to indicate blank spaces. Even rows are 95% HDI's, odd rows are MAP estimates or simulated hypothesis tests.

PrGL uses the group-level MAP estimates to simulate distributions (e.g. Norm(d_mu.gain, d_sd.gain) and Norm(d_mu.loss, d_sd.loss)), then calculates the probability that the distribution in the gain domain is greater than the distribution in the loss domain.

```{r groupmap}

######################
## Exploratory
######################

## load

source("set_e_dir.R")
load(file.path(tempdir, "aDDM-fit-gain.RData"))
load(file.path(tempdir, "aDDM-fit-loss.RData"))
trace.gain <- data.frame(combine.mcmc(fit.gain$mcmc))
trace.loss <- data.frame(combine.mcmc(fit.loss$mcmc))

## Get MAP estimates and their HDIs

MAP.group <- data.frame(
  Parameter = rep(NA,8),
  Gain = c(1:8),
  Loss = c(1:8),
  PrGL = rep(NA,8)
)

## Round the estimates and pull number of samples

rnd <- 2
samples <- 10000

# Gain: Mode and HDI

param <- 'd_mu'
MAP.group$Parameter[1] <- param
MAP.group$Gain[1]      <- estimate_mode(trace.gain[,param]) %>% round(rnd)
MAP.group$Loss[1]      <- estimate_mode(trace.loss[,param]) %>% round(rnd)
MAP.group$Gain[2]      <- estimate_hdi(trace.gain[,param]) %>% round(rnd) %>% toString()
MAP.group$Loss[2]      <- estimate_hdi(trace.loss[,param]) %>% round(rnd) %>% toString()

param <- 'sig_mu'
MAP.group$Parameter[3] <- param
MAP.group$Gain[3]      <- estimate_mode(trace.gain[,param]) %>% round(rnd)
MAP.group$Loss[3]      <- estimate_mode(trace.loss[,param]) %>% round(rnd)
MAP.group$Gain[4]      <- estimate_hdi(trace.gain[,param]) %>% round(rnd) %>% toString()
MAP.group$Loss[4]      <- estimate_hdi(trace.loss[,param]) %>% round(rnd) %>% toString()

param <- 'bias_mu'
MAP.group$Parameter[5] <- param
MAP.group$Gain[5]      <- estimate_mode(trace.gain[,param]*2-1) %>% round(rnd)
MAP.group$Loss[5]      <- estimate_mode(trace.loss[,param]*2-1) %>% round(rnd)
MAP.group$Gain[6]      <- estimate_hdi(trace.gain[,param]*2-1) %>% round(rnd) %>% toString()
MAP.group$Loss[6]      <- estimate_hdi(trace.loss[,param]*2-1) %>% round(rnd) %>% toString()

param <- 'theta_mu'
MAP.group$Parameter[7] <- param
MAP.group$Gain[7]      <- estimate_mode(trace.gain[,param]) %>% round(rnd)
MAP.group$Loss[7]      <- estimate_mode(trace.loss[,param]) %>% round(rnd)
MAP.group$Gain[8]      <- estimate_hdi(trace.gain[,param]) %>% round(rnd) %>% toString()
MAP.group$Loss[8]      <- estimate_hdi(trace.loss[,param]) %>% round(rnd) %>% toString()

## Hypothesis testing Gain > Loss

set.seed(seed)

d_mu.gain <- rnorm(samples, mean=estimate_mode(trace.gain$d_mu), sd=1/sqrt(estimate_mode(trace.gain$d_pr)))
d_mu.loss <- rnorm(samples, mean=estimate_mode(trace.loss$d_mu), sd=1/sqrt(estimate_mode(trace.loss$d_pr)))
MAP.group$PrGL[1] <- mean(d_mu.gain > d_mu.loss) %>% round(rnd)

sig_mu.gain <- rnorm(samples, mean=estimate_mode(trace.gain$sig_mu), sd=1/sqrt(estimate_mode(trace.gain$sig_pr)))
sig_mu.loss <- rnorm(samples, mean=estimate_mode(trace.loss$sig_mu), sd=1/sqrt(estimate_mode(trace.loss$sig_pr)))
MAP.group$PrGL[3] <- mean(sig_mu.gain > sig_mu.loss) %>% round(rnd)

bias.alpha.gain <- estimate_mode(trace.gain$bias_mu)*estimate_mode(trace.gain$bias_kappa)
bias.beta.gain <- (1-estimate_mode(trace.gain$bias_mu))*estimate_mode(trace.gain$bias_kappa)
bias.alpha.loss <- estimate_mode(trace.loss$bias_mu)*estimate_mode(trace.loss$bias_kappa)
bias.beta.loss <- (1-estimate_mode(trace.loss$bias_mu))*estimate_mode(trace.loss$bias_kappa)
bias_mu.gain <- rbeta(samples, bias.alpha.gain, bias.beta.gain)
bias_mu.loss <- rbeta(samples, bias.alpha.loss, bias.beta.loss)
MAP.group$PrGL[5] <- mean(bias_mu.gain > bias_mu.loss) %>% round(rnd)

theta_mu.gain <- rnorm(samples, mean=estimate_mode(trace.gain$theta_mu), sd=1/sqrt(estimate_mode(trace.gain$theta_pr)))
theta_mu.loss <- rnorm(samples, mean=estimate_mode(trace.loss$theta_mu), sd=1/sqrt(estimate_mode(trace.loss$theta_pr)))
MAP.group$PrGL[7] <- mean(theta_mu.gain > theta_mu.loss) %>% round(rnd)

# Save and display

save(MAP.group, file=file.path(outdir,"MAP-group.RData"))
MAP.group

```

Simulations of group-level estimates indicate with 99\% confidence that $\theta_{\mu}^{Loss}>\theta_{\mu}^{Gain}$.

## Comparison Tests

T-tests and Cohen's d. This is to appease the StatCheck gods.

```{r comparegroup}

######################
## Exploratory
######################

## load

source("set_e_dir.R")
load(file.path(tempdir, "aDDM-fit-gain.RData"))
load(file.path(tempdir, "aDDM-fit-loss.RData"))
trace.gain <- data.frame(combine.mcmc(fit.gain$mcmc))
trace.loss <- data.frame(combine.mcmc(fit.loss$mcmc))

## Get differences to test if away from 0

d_diff     <- trace.gain$d_mu-trace.loss$d_mu
sig_diff   <- trace.gain$sig_mu-trace.loss$sig_mu
bias_diff  <- trace.gain$bias_mu-trace.loss$bias_mu
theta_diff <- trace.gain$theta_mu-trace.loss$theta_mu

# T-tests

d.t     <- t.test(d_diff)
sig.t   <- t.test(sig_diff)
bias.t  <- t.test(bias_diff)
theta.t <- t.test(theta_diff)

# Cohen's d

d.cohen     <- cohen.d(trace.gain$d_mu,trace.loss$d_mu)
sig.cohen   <- cohen.d(trace.gain$sig_mu,trace.loss$sig_mu)
bias.cohen  <- cohen.d(trace.gain$bias_mu,trace.loss$bias_mu)
theta.cohen <- cohen.d(trace.gain$theta_mu,trace.loss$theta_mu)

# Display them

rnd <- 2

cat(
  " d_mu.gain - d_mu.loss \t\t | ",
  "t(",
  toString( round(d.t$parameter,rnd) ),
  ") = ",
  toString( round(d.t$statistic,rnd) ),
  " \t | d = ",
  toString( round(d.cohen$estimate,rnd) ),
  "\t (",
  toString(d.cohen$magnitude),
  ")",

  "\n sig_mu.gain - sig_mu.loss \t | ",
  "t(",
  toString( round(sig.t$parameter,rnd) ),
  ") = ",
  toString( round(sig.t$statistic,rnd) ),
  " \t | d = ",
  toString( round(sig.cohen$estimate,rnd) ),
  "\t (",
  toString(sig.cohen$magnitude),
  ")",
  
  "\n bias_mu.gain - bias_mu.loss \t | ",
  "t(",
  toString( round(bias.t$parameter,rnd) ),
  ") = ",
  toString( round(bias.t$statistic,rnd) ),
  " \t | d = ",
  toString( round(bias.cohen$estimate,rnd) ),
  "\t (",
  toString(bias.cohen$magnitude),
  ")",
  
  "\n theta_mu.gain - theta_mu.loss \t | ",
  "t(",
  toString( round(theta.t$parameter,rnd) ),
  ") = ",
  toString( round(theta.t$statistic,rnd) ),
  " \t | d = ",
  toString( round(theta.cohen$estimate,rnd) ),
  "\t (",
  toString(theta.cohen$magnitude),
  ")"
)

```

T-tests and Cohen's d indicate that the change in theta is **large**.

--------------------------------------------------------------------------------------------------------

# Out-of-Sample {.tabset}

### Simulate Trial

Write a function that simulates behavior (choice, rt) for a single trial using the aDDM. It will take as inputs parameters to the aDDM and fixation properties (e.g. probability of first fixating left which is a scalar, or middle fixations which is the distribution of all middle fixations for that subject from the testing set).

This is code taken from Eum, Dolbier, and Rangel (2022), which is code HEAVILY based on Lombardi and Hare (2021) and Tavares, Perona, and Rangel (2017).

Note that I am not modeling saccades ("transitions") here. Instead I assume it is being captured by non-decision time in the form of latency to first fixation.

```{r simtrial}

sim.trial <- function(b, d, t, s, vL, vR, prFirstLeft, firstFix, middleFix, latency) {
  
  ###################################################################
  # create a variable to track when to stop (for efficiency purposes)
  stopper <- 0
  
  ##############################
  # initialize rdv at bias point
  RDV <- b
  rt  <- 0
  
  ##############################
  # latency to first fixation
  latencyDur <- sample(latency,1)
  latency_err <- rnorm(n=latencyDur, mean=0, sd=s)
  
  if (abs(RDV+sum(latency_err))>=1) {
    for (t in 1:latencyDur) {
      RDV <- RDV + latency_err[t]
      rt <- rt + 1
      lastLoc <- 4
      if (abs(RDV)>=1) {stopper<-1; break}
    }
  } else {
    RDV <- RDV + sum(latency_err)
    rt <- rt + latencyDur
  }
  
  ##############################
  # first fixation
  if (stopper==0) {
    firstDur <- sample(firstFix,1)
    loc <- rbinom(1,1,prFirstLeft)
    if (loc==1) {drift_mean <- (d)*(vL-t*vR)}
    if (loc==0) {drift_mean <- (d)*(t*vL-vR)}
    drift <- drift_mean + rnorm(n=firstDur, mean=0, sd=s)
  
    if (abs(RDV+sum(drift))>=1) {
      for (t in 1:firstDur) {
        RDV <- RDV + drift[t]
        rt <- rt + 1
        lastLoc <- loc
        if (abs(RDV)>=1) {stopper<-1; break}
      }
    } else {
      RDV <- RDV + sum(drift)
      rt <- rt + firstDur
      prevLoc <- loc
    }
  }
  
  #######################################################
  # middle fixations until choice is made
  while (abs(RDV)<1) {

    if (stopper==0) {
      middleDur <- sample(middleFix,1)
      if (prevLoc==1) {loc<-0}
      if (prevLoc==0) {loc<-1}
      if (loc==1) {drift_mean <- (d)*(vL-t*vR)}
      if (loc==0) {drift_mean <- (d)*(t*vL-vR)}
      drift <- drift_mean + rnorm(n=middleDur, mean=0, sd=s)
    
      if (abs(RDV+sum(drift))>=1) {
        for (t in 1:middleDur) {
          RDV <- RDV + drift[t]
          rt <- rt + 1
          lastLoc <- loc
          if (abs(RDV)>=1) {break}
        }
      } else {
        RDV <- RDV + sum(drift)
        rt <- rt + middleDur
        prevLoc <- loc
      }
    }
  }

  ##############################
  # return your results
  if (RDV>0) {choice <- 1}
  if (RDV<0) {choice <- 0}
  vDiff <- round(vL-vR, 1) #have to round, otherwise there are miniscule differences that generate extra rows in pdata...
  results <- data.frame(choice=choice, rt=rt, vL=vL, vR=vR, vDiff=vDiff, lastFix=lastLoc)
  
  return(results)
}

```

## Simulate Dataset

Write a function that takes in testing data and spits out plot data.

```{r simpdata}
sim.pdata <- function(test, trace, simCount) {

  # Placeholders
  vL <- list()
  vR <- list()
  d <- list()
  sig <- list()
  theta <- list()
  bias <- list()
  prFirstLeft <- list()
  ndt <- list()
  firstFix <- list()
  transition <- list()
  middleFix <- list()
  
  sim <- data.frame(
    subject = NA,
    trial = NA,
    choice = NA,
    rt = NA,
    vL = NA,
    vR = NA,
    vDiff = NA,
    simulation = NA,
    lastFix = NA
  )
  
  sims <- list()
  
  # Loop though gain-loss domains
  ind <- 0
  for (cond in c("Gain","Loss")) {
    
    # Loop through subjects to get everyone's choices and RT
    for (sub in sort(unique(test$subject))) {
      
      # Progress tracker
      #print(paste0(cond,sub))
      
      # gather relevant data
      subjdata <- test[test$subject==sub & test$Condition==cond & test$fix_type=="First",]
      vL[[sub]] <- subjdata$vL
      vR[[sub]] <- subjdata$vR
      
      d.ind <- paste0('d.',toString(sub),'.')
      g.ind <- paste0('g.',toString(sub),'.')
      sig.ind <- paste0('sig.',toString(sub),'.')
      bias.ind <- paste0('bias.',toString(sub),'.')
      theta.ind <- paste0('theta.',toString(sub),'.')
      
      # convert all parameters to 1 ms timestep (originally in 1 s timestep)
      d[[sub]]     <- sample(trace[,d.ind], simCount)/1000
      sig[[sub]]   <- sample(trace[,sig.ind], simCount)/sqrt(1000)
      bias[[sub]]  <- sample(trace[,bias.ind], simCount)*2-1
      theta[[sub]] <- sample(trace[,theta.ind], simCount) 
      
      # fixation properties
      subjfix <- test[test$subject==sub & test$Condition==cond,]
      prFirstLeft[[sub]] <- mean( (subjfix$Location[subjfix$fix_type=="First"]==1) ,na.rm=T)
      ndt[[sub]] <- abs(test$ndt) #is being modeled as latency to first fixation, but captures all ndt  (! ! !) get rid of abs when ndt fixed
      firstFix[[sub]] <- round(subjfix$fix_dur[subjfix$fix_type=="First"],0)
      transition[[sub]] <- c(0) #no transitions. this is captured by non-decision time
      middleFix[[sub]] <- round(subjfix$fix_dur[subjfix$fix_type=="Middle"],0)
      
      # Simulate
      for (j in 1:simCount) {
        for (i in 1:length(vL[[sub]])) {
          
          simTrial <- sim.trial(
              b = bias[[sub]][j],
              d = d[[sub]][j],
              t = theta[[sub]][j],
              s = sig[[sub]][j],
              vL = vL[[sub]][i],
              vR = vR[[sub]][i],
              prFirstLeft = prFirstLeft[[sub]],
              latency = ndt[[sub]],
              firstFix = firstFix[[sub]],
              middleFix = middleFix[[sub]]
          )
          
          simTrial$subject <- sub
          simTrial$trial <- i
          simTrial$simulation <- j
          sim <- rbind(sim, simTrial)
        }
      }
      
    }
    sim <- na.omit(sim)
    ind <- ind + 1
    sims[[ind]] <- sim
    
  }
  
  pdata.real.gain <- test[test$Condition=="Gain",] %>%
    group_by(subject, vDiff) %>%
    summarize(
      choices = mean(choice, na.rm=T),
      rts = mean(rt, na.rm=T)
    ) %>%
    ungroup() %>%
    group_by(vDiff) %>%
    summarize(
      choice.mean = mean(choices, na.rm=T),
      choice.se = std.error(choices, na.rm=T),
      rt.mean = mean(rts, na.rm=T),
      rt.se = std.error(rts, na.rm=T),
      Dataset = 0,
      Condition = "Gain"
    )
  pdata.real.loss <- test[test$Condition=="Loss",] %>%
    group_by(subject, vDiff) %>%
    summarize(
      choices = mean(choice, na.rm=T),
      rts = mean(rt, na.rm=T)
    ) %>%
    ungroup() %>%
    group_by(vDiff) %>%
    summarize(
      choice.mean = mean(choices, na.rm=T),
      choice.se = std.error(choices, na.rm=T),
      rt.mean = mean(rts, na.rm=T),
      rt.se = std.error(rts, na.rm=T),
      Dataset = 0,
      Condition = "Loss"
    )
  pdata.sim.gain <- sims[[1]] %>%
    group_by(simulation, subject, vDiff) %>%
    summarize(
      choices = mean(choice, na.rm=T),
      rts = mean(rt, na.rm=T)
    ) %>%
    ungroup() %>%
    group_by(subject, vDiff) %>%
    summarize(
      choices = mean(choices, na.rm=T),
      rts = mean(rts, na.rm=T)
    ) %>%
    group_by(vDiff) %>%
    summarize(
      choice.mean = mean(choices, na.rm=T),
      choice.se = std.error(choices, na.rm=T),
      rt.mean = mean(rts, na.rm=T),
      rt.se = std.error(rts, na.rm=T),
      Dataset = 1,
      Condition = "Gain"
    )
  pdata.sim.loss <- sims[[2]] %>%
    group_by(simulation, subject, vDiff) %>%
    summarize(
      choices = mean(choice, na.rm=T),
      rts = mean(rt, na.rm=T)
    ) %>%
    ungroup() %>%
    group_by(subject, vDiff) %>%
    summarize(
      choices = mean(choices, na.rm=T),
      rts = mean(rts, na.rm=T)
    ) %>%
    group_by(vDiff) %>%
    summarize(
      choice.mean = mean(choices, na.rm=T),
      choice.se = std.error(choices, na.rm=T),
      rt.mean = mean(rts, na.rm=T),
      rt.se = std.error(rts, na.rm=T),
      Dataset = 1,
      Condition = "Loss"
    )
  
  pdata <- bind_rows(pdata.real.gain, pdata.real.loss, pdata.sim.gain, pdata.sim.loss)
  pdata$Dataset <- factor(pdata$Dataset, levels=c(0,1), labels=c("Data","Simulation"))
 
  return(pdata) 
}

```

## Group Predictions

I want two plots, each with 2 lines. One plot for choices. One plot for RTs. One line will show true out-of-sample data. One line will show aDDM predictions for those exact same trials. Run 20 simulations.

```{r grouppred, cache=TRUE, message=FALSE}

## How many simulations to run

simCount = 20

######################
## Exploratory
######################

## load data

source("set_e_dir.R")
source("load_data.R")
load(file.path(tempdir, "aDDM-fit-gain.RData"))
load(file.path(tempdir, "aDDM-fit-loss.RData"))
trace.gain <- data.frame(combine.mcmc(fit.gain$mcmc))
trace.loss <- data.frame(combine.mcmc(fit.loss$mcmc))

## Simulate pdata using function from above

if (quickmodel==FALSE) {
  pdata <- sim.pdata(cfr_even, trace.gain, simCount)

## Save the simulations and code in some quick skips

  save(pdata, file=file.path(tempdir,"simulation_pdata.RData"))
} else { load(file.path(tempdir,"simulation_pdata.RData")) }

## Plot the pdata!

plt.choice.sim.gain <- ggplot(data=pdata[pdata$Condition=="Gain",], aes(x=vDiff)) +
  geom_hline(yintercept=0.5, color='grey', alpha=0.75) +
  geom_vline(xintercept=0, color='grey', alpha=0.75) +
  geom_line(aes(y=choice.mean, group=Dataset, color=Dataset), size=2) +
  geom_ribbon(aes(ymin=choice.mean-choice.se, ymax=choice.mean+choice.se, group=Dataset, fill=Dataset), alpha=.2) +
  ylab("Pr(choose left)") +
  xlab("") +
  ylim(c(0,1)) +
  theme_classic() +
  theme(
    legend.position=c(0.25,.8)
  ) +
  scale_color_uchicago() +
  scale_fill_uchicago()

plt.rt.sim.gain <- ggplot(data=pdata[pdata$Condition=="Gain",], aes(x=vDiff)) +
  geom_vline(xintercept=0, color='grey', alpha=0.75) +
  geom_line(aes(y=rt.mean, group=Dataset, color=Dataset), size=2) +
  geom_ribbon(aes(ymin=rt.mean-rt.se, ymax=rt.mean+rt.se, group=Dataset, fill=Dataset), alpha=.2) +
  ylab("Response time (s)") +
  xlab("Value difference (L-R)") +
  ylim(c(0,NA)) +
  theme_classic() +
  theme(
    legend.position="None"
  ) +
  scale_color_uchicago() +
  scale_fill_uchicago()

plt.choice.sim.loss <- ggplot(data=pdata[pdata$Condition=="Loss",], aes(x=vDiff)) +
  geom_hline(yintercept=0.5, color='grey', alpha=0.75) +
  geom_vline(xintercept=0, color='grey', alpha=0.75) +
  geom_line(aes(y=choice.mean, group=Dataset, color=Dataset), size=2) +
  geom_ribbon(aes(ymin=choice.mean-choice.se, ymax=choice.mean+choice.se, group=Dataset, fill=Dataset), alpha=.2) +
  ylab("") +
  xlab("") +
  ylim(c(0,1)) +
  theme_classic() +
  theme(
    legend.position="None"
  ) +
  scale_color_uchicago() +
  scale_fill_uchicago()

plt.rt.sim.loss <- ggplot(data=pdata[pdata$Condition=="Loss",], aes(x=vDiff)) +
  geom_vline(xintercept=0, color='grey', alpha=0.75) +
  geom_line(aes(y=rt.mean, group=Dataset, color=Dataset), size=2) +
  geom_ribbon(aes(ymin=rt.mean-rt.se, ymax=rt.mean+rt.se, group=Dataset, fill=Dataset), alpha=.2) +
  ylab("") +
  xlab("Value difference (L-R)") +
  ylim(c(0,NA)) +
  theme_classic() +
  theme(
    legend.position="None"
  ) +
  scale_color_uchicago() +
  scale_fill_uchicago()

## Combine the plots with fancy backgrouns too!

plt.simulations <- grid.arrange(
  arrangeGrob(
    plt.choice.sim.gain + theme(plot.background = element_rect(fill = 'lightcyan', color = 'lightcyan')), 
    plt.rt.sim.gain + theme(plot.background = element_rect(fill = 'lightcyan', color = 'lightcyan')),
    nrow=2,
    top=text_grob("Gain", size=14, face=2)
  ),
  arrangeGrob(
    plt.choice.sim.loss + theme(plot.background = element_rect(fill = 'mistyrose', color = 'mistyrose')), 
    plt.rt.sim.loss + theme(plot.background = element_rect(fill = 'mistyrose', color = 'mistyrose')),
    nrow=2,
    top=text_grob("Loss", size=14, face=2)
  ),
  ncol=2
)

```

Yikes. Need to revisit the drawing board. This is unacceptable.
I feel like drift rate might be too large.

--------------------------------------------------------------------------------------------------------

# Additional Fix. Prop. {.tabset}

## Pr(First Fix. Left)

```{r firstleft, message=FALSE, warning=FALSE}

addfixprop.firstleft <- function(data) {
  
  pdata <- data[data$fix_type=="First",] %>%
    group_by(subject, Condition) %>%
    summarize(
      prleft.mean = mean( ifelse(Location=="Left",1,0) , na.rm=T) 
    ) %>%
    ungroup() %>%
    group_by(Condition) %>%
    summarize(
      y = mean(prleft.mean, na.rm=T),
      se = std.error(prleft.mean, na.rm=T)
    )
  
  plt <- ggplot(data=pdata, aes(x=Condition, y=y)) +
    geom_bar(aes(fill=Condition), position=position_dodge(.9), stat="identity") +
    geom_errorbar(aes(ymin=y-se, ymax=y+se), position=position_dodge(.9), color='black', width=markersize) +
    ylim(c(0,1)) +
    labs(y="Pr(first fix. left)", x="Condition")
    
  
  return(plt)
}

######################
## Exploratory
######################

source("set_e_dir.R")
source("load_data.R")

plt.firstleft.e <- addfixprop.firstleft(cfr)
plt.firstleft.e

```

Probability of first fixating left is around 75\% for both conditions.
This is consistent with previous findings.

--------------------------------------------------------------------------------------------------------

# Methods {.tabset}

## Task

To investigate whether the value of nonfixated items are discounted or amplified in different domains, we run a binary choice task with monetary lotteries.
400 trials are split into 2 randomly-ordered blocks with different conditions: (1) a gain condition in which lottery outcomes are \$10 or \$0, and (2) a loss condition in which lottery outcomes are \$-10 or \$0.

Lotteries are presented in the form of 2 circles, one on the left and right sides of the screen. 
Each circle contains 100 dots.
In the gain condition, the dots are green or white.
In the loss condition, the dots are red or white.
White dots correspond to the probability of earning nothing, green dots correspond to the probability of earning \$10, and red dots correspond to the probability of losing \$10.
The probability of each win or loss is selected independently from \{45%, 46%, ..., 54%, 55%\}.

Each trial begins with a fixation cross that the subject must fixate on for 500 ms.
After, 2 lotteries are presented on the screen.
Subjects are given free response time to choose.
Once a choice is made, a feedback screen highlights that choice with a red square for 1 s.
Trials are separated by a blank inter-trial-interval of 1 s.

Subjects are paid a \$40 show-up fee. 
For both the gain and loss blocks, subjects draw a number between 1 and 200 from an urn and are paid according to the outcome of the selected lottery in that trial.
Gains are added to and losses are subtracted from the show-up fee.

## Participants

We collected 60 subjects from in and around the Caltech community.
Each subject makes 400 choices, leaving plenty of data for computational modeling of choices and response times.
The experiment was approved by Caltech's IRB.

## Eye-tracking

We recorded subjects' fixation patterns at 500 Hz using an EyeLink 100.
Subjects sat about 60 cm away from a 1920 by 1080 pixel monitor.
Lottery circles had a radius of 300 pixels.
Fixations within the ROI of the left lottery were recorded as "left", while fixations to the right lottery were recorded as "right".
Occasionally, due to blinks or eye-tracker noise, fixation patterns exhibit transient "blank" recordings between two fixations to the same ROI (e.g. left-blank-left).
We recode these as a single fixation to the same ROI (e.g. left-left-left).
Transient "blank" recordings between two fixations to different ROI (e.g. left-blank-right) are recorded as saccades between fixations.

## Inference Strategy

To avoid the rigidity of pre-registering without compromising on statistical integrity, we adopt the exploration-replication approach laid out in Eum, Dolbier, and Rangel (2022).
The first 30 of the 60 subjects were allocated to an exploratory dataset, while the remaining subjects were allocated to a confirmatory dataset.
We refrain from looking at any data in the confirmatory dataset until we have explored and pinned down a set of analyses in the exploratory dataset. 
Pre-committed analysis of the confirmatory dataset allows us to (hopefully!) replicate the results from the exploratory dataset and obtain unbiased statistics for hypothesis testing.
If the results across the two datasets are similar, we also pool the data and report those results, as if it were a meta-analysis.
We call these 3 separate datasets the exploratory, confirmatory, and joint datasets.

## Computational Model

We use the Attentional Drift-Diffusion-Model (aDDM) to analyze choices and response times in the data.
The model initializes an evidence variable at some starting point, $\text{Evidence}_{t=0}=b$ (bias). 
As the subject observes the stimuli, they gather and integrate evidence in favor of one option or the other until this evidence variable hits a decision boundary, fixed at $\pm 1$:

$\text{Evidence}_t = \text{Evidence}_{t-1} + \mu_t + \epsilon_t$

$\epsilon_t \sim N(0,\sigma^2)$ is i.i.d. white Gaussian noise that captures noise in the decision process.
If the subject is looking left at time $t$, then $\mu_t=d(E[V_L]-\theta E[V_R])$.
If the subject is looking right, then $\mu_t=d(\theta E[V_L]-E[V_R])$.
$E[V_k], k\in\{L,R\}$ is the expected value of the left and right lotteries, respectively.
$d$ is the drift rate parameter, reflecting the amount of evidence the subject accumulates per discrete time-step (e.g. 1 ms).
$\theta$ is traditionally thought of as the attentional discounting parameter.
It reflects how subjects will over-weight the value of the currently fixated option.
In the gain domain ($V_k\geq0$), this translates to a discount factor on the value of the nonfixated option (i.e. $\theta \in (0,1)$).
However in the loss domain ($V_k<0$), we hypothesize that $\theta$ instead acts as an attentional amplification of the value of the currently fixated option (i.e. $\theta > 1$).
The aDDM also nests the standard DDM by allowing $theta =1$, implying no attentional biases.

Note that for every trial, the aDDM assumes that the fixation process is independent of the current state of evidence.

## aDDM Fitting

We fit the aDDM separately to gain and loss trials using the hierarchical Bayesian model approach developed by Lombardi and Hare (2021).
This is done separately in the exploratory, confirmatory, and joint datasets.
Odd trials are used to obtain estimates for the model parameters, and even trials are used to test out-of-sample predictions of the model.
Decision boundaries are fixed at $\pm 1$, so the model contains 4 free parameters: drift rate ($d$), noise variance ($\sigma^2$), starting point bias ($b$), and attentional bias ($\theta$).
We use Markov Chain Monte Carlo methods to estimate posterior distributions (3 chains, 30,000 warm-up samples, and 30,000 samples from each of the posteriors).


--------------------------------------------------------------------------------------------------------

# S. Methods {.tabset}

## Out-of-sample Sim.

Data from even trials are set aside to compare with predictions from the aDDM fitted on odd trials.
We simulate 20 different datasets using different samples from aDDM parameter posteriors and report the mean and standard error across these simulations.
Fixation patterns are sampled from observed distributions in the even trials. 
For instance, we calculate the proportion of the even trials that the subject looks left first and use that to probabilistically determine first fixation location in our simulations.
Simulated first fixation durations are sampled from the distribution of first fixation durations in the even trials, and simluated middle fixation durations are sampled from middle fixation distributions for all simulated fixations after the first.
If the evidence variable reaches a decision boundary before a fixation duration has fully elapsed, then the process is terminated and the choice and response time are recorded.
Otherwise, a simulated saccade duration is sampled and the process continues.
Note that we make an unrealistic assumption that the expected value of the nonfixated lottery is known during the first fixation, which obstructs the accuracy of our predictions.

## Hierarchical Reg.

We use the brms R-package (Burkner, 2017, 2018) to run hierarchical linear and logistic regressions.
We used the default, weakly informative priors where appropriate, and only scaled the priors depending on the units of the independent variable. 
Posterior distributions were obtained using 3 chains for a total of 9,000 warm-up samples and 9,000 samples from each of the posteriors.